# Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†
# ÙØ§Ø² 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Classification Models)

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings

warnings.filterwarnings('ignore')

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             confusion_matrix, classification_report, roc_curve, auc,
                             roc_auc_score)

# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

# Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ…
from sklearn.tree import plot_tree

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (12, 8)

print("=" * 80)
print("ğŸ¯ ÙØ§Ø² 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Classification Models)")
print("=" * 80)

# ========================
# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø§Ø² ÙØ§Ø² 1")
print("=" * 60)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ ØªØ³Øª
train_data = pd.read_csv('train_data.csv')
test_data = pd.read_csv('test_data.csv')

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
with open('preprocessing_info.pkl', 'rb') as f:
    preprocessing_info = pickle.load(f)

feature_columns = preprocessing_info['feature_columns']

# Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
X_train = train_data[feature_columns]
y_train = train_data['Pass_Status_Encoded']
X_test = test_data[feature_columns]
y_test = test_data['Pass_Status_Encoded']

print(f"\nâœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯:")
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ: {X_train.shape[0]}")
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: {X_test.shape[0]}")
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_train.shape[1]}")

# ========================
# 2. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ¹Ø±ÛŒÙ Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),
    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),
    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'Neural Network': MLPClassifier(random_state=42, max_iter=1000)
}

print("\nğŸ¤– Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡:")
for i, model_name in enumerate(models.keys(), 1):
    print(f"  {i}. {model_name}")

# ========================
# 3. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Cross-Validation
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 3: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Cross-Validation")
print("=" * 60)

# ØªØ¹Ø±ÛŒÙ StratifiedKFold Ø¨Ø±Ø§ÛŒ Cross-Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
results = {}
trained_models = {}

print("\nğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§...")
print("â”€" * 50)

for model_name, model in models.items():
    print(f"\nğŸ”„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„: {model_name}")

    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
    model.fit(X_train, y_train)
    trained_models[model_name] = model

    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
    y_pred = model.predict(X_test)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Cross-Validation scores
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC-ROC Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ probability Ø¯Ø§Ø±Ù†Ø¯
    try:
        y_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_proba)
    except:
        auc_score = None

    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    results[model_name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'auc': auc_score,
        'y_pred': y_pred,
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }

    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    print(f"  âœ“ Accuracy: {accuracy:.4f}")
    print(f"  âœ“ Precision: {precision:.4f}")
    print(f"  âœ“ Recall: {recall:.4f}")
    print(f"  âœ“ F1-Score: {f1:.4f}")
    print(f"  âœ“ Cross-Val Mean: {cv_mean:.4f} (Â±{cv_std:.4f})")
    if auc_score:
        print(f"  âœ“ AUC-ROC: {auc_score:.4f}")

# ========================
# 4. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 4: Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
print("=" * 60)

# Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
comparison_data = []
for model_name, metrics in results.items():
    comparison_data.append({
        'Model': model_name,
        'Accuracy': metrics['accuracy'],
        'Precision': metrics['precision'],
        'Recall': metrics['recall'],
        'F1-Score': metrics['f1_score'],
        'CV Mean': metrics['cv_mean'],
        'CV Std': metrics['cv_std'],
        'AUC': metrics['auc'] if metrics['auc'] else 0
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('F1-Score', ascending=False)

print("\nğŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ F1-Score):")
print("â”€" * 80)
print(comparison_df.to_string(index=False))

# Ø°Ø®ÛŒØ±Ù‡ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
comparison_df.to_csv('model_comparison.csv', index=False, encoding='utf-8-sig')
print("\nâœ… Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ 'model_comparison.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# ========================
# 5. Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 5: Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„")
print("=" * 60)

# Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ F1-Score
best_model_name = comparison_df.iloc[0]['Model']
best_model = trained_models[best_model_name]
best_metrics = results[best_model_name]

print(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model_name}")
print("â”€" * 40)
print(f"  â€¢ Accuracy: {best_metrics['accuracy']:.4f}")
print(f"  â€¢ Precision: {best_metrics['precision']:.4f}")
print(f"  â€¢ Recall: {best_metrics['recall']:.4f}")
print(f"  â€¢ F1-Score: {best_metrics['f1_score']:.4f}")
print(f"  â€¢ Cross-Validation: {best_metrics['cv_mean']:.4f} (Â±{best_metrics['cv_std']:.4f})")

# ========================
# 6. ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 6: ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„")
print("=" * 60)

# Confusion Matrix
print(f"\nğŸ“Š Confusion Matrix Ø¨Ø±Ø§ÛŒ {best_model_name}:")
print(best_metrics['confusion_matrix'])

# Classification Report
y_pred_best = best_metrics['y_pred']
print(f"\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred_best,
                            target_names=['Ø±Ø¯ (Fail)', 'Ù‚Ø¨ÙˆÙ„ (Pass)']))

# ========================
# 7. Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 7: Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ")
print("=" * 60)

# Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Ù†Ù…ÙˆØ¯Ø§Ø± Accuracy
ax1 = axes[0, 0]
models_list = comparison_df['Model'].tolist()
accuracies = comparison_df['Accuracy'].tolist()
bars1 = ax1.bar(range(len(models_list)), accuracies, color='skyblue')
ax1.set_xlabel('Models')
ax1.set_ylabel('Accuracy')
ax1.set_title('Accuracy Comparison')
ax1.set_xticks(range(len(models_list)))
ax1.set_xticklabels(models_list, rotation=45, ha='right')
ax1.grid(True, alpha=0.3)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±
for i, v in enumerate(accuracies):
    ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

# 2. Ù†Ù…ÙˆØ¯Ø§Ø± F1-Score
ax2 = axes[0, 1]
f1_scores = comparison_df['F1-Score'].tolist()
bars2 = ax2.bar(range(len(models_list)), f1_scores, color='lightgreen')
ax2.set_xlabel('Models')
ax2.set_ylabel('F1-Score')
ax2.set_title('F1-Score Comparison')
ax2.set_xticks(range(len(models_list)))
ax2.set_xticklabels(models_list, rotation=45, ha='right')
ax2.grid(True, alpha=0.3)

for i, v in enumerate(f1_scores):
    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

# 3. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ù…Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
ax3 = axes[1, 0]
metrics_comparison = comparison_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values
x = np.arange(len(models_list))
width = 0.2

for i, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1-Score']):
    offset = width * (i - 1.5)
    ax3.bar(x + offset, metrics_comparison[:, i], width, label=metric)

ax3.set_xlabel('Models')
ax3.set_ylabel('Score')
ax3.set_title('All Metrics Comparison')
ax3.set_xticks(x)
ax3.set_xticklabels(models_list, rotation=45, ha='right')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Ù†Ù…ÙˆØ¯Ø§Ø± Cross-Validation Mean
ax4 = axes[1, 1]
cv_means = comparison_df['CV Mean'].tolist()
cv_stds = comparison_df['CV Std'].tolist()
bars4 = ax4.bar(range(len(models_list)), cv_means, yerr=cv_stds,
                color='coral', capsize=5)
ax4.set_xlabel('Models')
ax4.set_ylabel('Cross-Validation Score')
ax4.set_title('Cross-Validation Mean Score with Std Dev')
ax4.set_xticks(range(len(models_list)))
ax4.set_xticklabels(models_list, rotation=45, ha='right')
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('model_comparison_charts.png', dpi=100, bbox_inches='tight')
print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± 'model_comparison_charts.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# ========================
# 8. Confusion Matrix Heatmap Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§
# ========================
print("\nğŸ“Š Ø§ÛŒØ¬Ø§Ø¯ Confusion Matrix Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§...")

fig, axes = plt.subplots(3, 3, figsize=(15, 15))
axes = axes.ravel()

for idx, (model_name, metrics) in enumerate(results.items()):
    if idx < 9:
        cm = metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    ax=axes[idx], cbar=False)
        axes[idx].set_title(f'{model_name}')
        axes[idx].set_xlabel('Predicted')
        axes[idx].set_ylabel('Actual')
        axes[idx].set_xticklabels(['Fail', 'Pass'])
        axes[idx].set_yticklabels(['Fail', 'Pass'])

plt.suptitle('Confusion Matrices for All Models', fontsize=16)
plt.tight_layout()
plt.savefig('confusion_matrices_all.png', dpi=100, bbox_inches='tight')
print("âœ… Confusion Matrices Ø¯Ø± 'confusion_matrices_all.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# ========================
# 9. ROC Curves
# ========================
print("\nğŸ“Š Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC...")

plt.figure(figsize=(10, 8))

for model_name, model in trained_models.items():
    try:
        y_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        auc_score = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')
    except:
        pass

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.savefig('roc_curves.png', dpi=100, bbox_inches='tight')
print("âœ… Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC Ø¯Ø± 'roc_curves.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# ========================
# 10. Feature Importance (Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Tree-based)
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 8: ØªØ­Ù„ÛŒÙ„ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
print("=" * 60)

# Ø§Ú¯Ø± Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Tree-based Ø¨Ø§Ø´Ø¯
if best_model_name in ['Decision Tree', 'Random Forest', 'Gradient Boosting']:
    feature_importance = best_model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': feature_columns,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False)

    print(f"\nğŸ“Š Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {best_model_name}:")
    print(feature_importance_df.to_string(index=False))

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
    plt.xlabel('Importance')
    plt.title(f'Feature Importance - {best_model_name}')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=100, bbox_inches='tight')
    print("\nâœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± 'feature_importance.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    plt.close()

# ========================
# 11. Hyperparameter Tuning Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 9: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„")
print("=" * 60)

print(f"\nğŸ”§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ {best_model_name}...")

# ØªØ¹Ø±ÛŒÙ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Grid Search
param_grids = {
    'Decision Tree': {
        'max_depth': [3, 5, 7, 10, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    },
    'Logistic Regression': {
        'C': [0.01, 0.1, 1, 10, 100],
        'penalty': ['l2'],
        'solver': ['lbfgs', 'liblinear']
    },
    'SVM (RBF)': {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto', 0.001, 0.01]
    }
}

if best_model_name in param_grids:
    param_grid = param_grids[best_model_name]

    # Ø§Ù†Ø¬Ø§Ù… Grid Search
    grid_search = GridSearchCV(
        estimator=models[best_model_name],
        param_grid=param_grid,
        cv=cv,
        scoring='f1_weighted',
        n_jobs=-1,
        verbose=0
    )

    grid_search.fit(X_train, y_train)

    print(f"\nâœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:")
    for param, value in grid_search.best_params_.items():
        print(f"  â€¢ {param}: {value}")

    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
    y_pred_optimized = grid_search.predict(X_test)

    accuracy_opt = accuracy_score(y_test, y_pred_optimized)
    precision_opt = precision_score(y_test, y_pred_optimized, average='weighted')
    recall_opt = recall_score(y_test, y_pred_optimized, average='weighted')
    f1_opt = f1_score(y_test, y_pred_optimized, average='weighted')

    print(f"\nğŸ“Š Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡:")
    print(f"  â€¢ Accuracy: {accuracy_opt:.4f} (Ù‚Ø¨Ù„ÛŒ: {best_metrics['accuracy']:.4f})")
    print(f"  â€¢ Precision: {precision_opt:.4f} (Ù‚Ø¨Ù„ÛŒ: {best_metrics['precision']:.4f})")
    print(f"  â€¢ Recall: {recall_opt:.4f} (Ù‚Ø¨Ù„ÛŒ: {best_metrics['recall']:.4f})")
    print(f"  â€¢ F1-Score: {f1_opt:.4f} (Ù‚Ø¨Ù„ÛŒ: {best_metrics['f1_score']:.4f})")

    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
    best_model_optimized = grid_search.best_estimator_
else:
    best_model_optimized = best_model

# ========================
# 12. Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 10: Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù†ØªØ§ÛŒØ¬")
print("=" * 60)

# Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
with open('best_model.pkl', 'wb') as f:
    pickle.dump({
        'model': best_model_optimized,
        'model_name': best_model_name,
        'metrics': best_metrics,
        'feature_columns': feature_columns
    }, f)

print(f"\nâœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ ({best_model_name}) Ø¯Ø± ÙØ§ÛŒÙ„ 'best_model.pkl' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
with open('all_models.pkl', 'wb') as f:
    pickle.dump(trained_models, f)

print("âœ… ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ 'all_models.pkl' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ ØªÙØµÛŒÙ„ÛŒ
with open('classification_results.pkl', 'wb') as f:
    pickle.dump(results, f)

print("âœ… Ù†ØªØ§ÛŒØ¬ ØªÙØµÛŒÙ„ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ 'classification_results.pkl' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# ========================
# 13. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
# ========================
print("\n" + "=" * 80)
print("âœ… ÙØ§Ø² 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
print("=" * 80)

print("\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡:")
print("â”€" * 40)
print("1. âœ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡")
print("2. âœ“ Ø¢Ù…ÙˆØ²Ø´ 9 Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø®ØªÙ„Ù")
print("3. âœ“ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Cross-Validation (5-Fold)")
print("4. âœ“ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Accuracy, Precision, Recall, F1-Score")
print("5. âœ“ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
print(f"6. âœ“ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model_name}")
print("7. âœ“ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„")
print("8. âœ“ ØªØ­Ù„ÛŒÙ„ Feature Importance")
print("9. âœ“ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ")

print("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
print("â”€" * 40)
print("  â€¢ model_comparison.csv - Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
print("  â€¢ best_model.pkl - Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡")
print("  â€¢ all_models.pkl - ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡")
print("  â€¢ classification_results.pkl - Ù†ØªØ§ÛŒØ¬ ØªÙØµÛŒÙ„ÛŒ")
print("  â€¢ model_comparison_charts.png - Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡")
print("  â€¢ confusion_matrices_all.png - Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ")
print("  â€¢ roc_curves.png - Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC")
print("  â€¢ feature_importance.png - Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")

print("\nğŸ† Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:")
print("â”€" * 40)
print(f"Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model_name}")
print(f"F1-Score: {best_metrics['f1_score']:.4f}")
print(f"Accuracy: {best_metrics['accuracy']:.4f}")

print("\nğŸ¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø²Ù‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ:")
print("â”€" * 40)
print("  â†’ Clustering (Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)")
print("  â†’ Association Rules (Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)")

print("\n" + "=" * 80)
print("Ù¾Ø§ÛŒØ§Ù† ÙØ§Ø² 2 - Classification Models ğŸš€")
print("=" * 80)