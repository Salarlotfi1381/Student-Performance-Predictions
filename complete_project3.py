# Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†
# ÙØ§Ø² 3: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† (Clustering Students)

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings

warnings.filterwarnings('ignore')

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import cdist

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_style("whitegrid")

print("=" * 80)
print("ðŸŽ¯ ÙØ§Ø² 3: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† (Clustering Students)")
print("=" * 80)

# ========================
# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡")
print("=" * 60)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
df = pd.read_csv('processed_student_data.csv')

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
with open('preprocessing_info.pkl', 'rb') as f:
    preprocessing_info = pickle.load(f)

feature_columns = preprocessing_info['feature_columns']

# Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
X_clustering = df[feature_columns]
y_actual = df['Pass_Status_Encoded']  # Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§

print(f"\nâœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯:")
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {X_clustering.shape[0]}")
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_clustering.shape[1]}")
print(f"\nÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
for i, col in enumerate(feature_columns, 1):
    print(f"  {i}. {col}")

# ========================
# 2. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 2: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clustering)

print("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ØªØ±")

# Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ visualizatio Ø¨Ø§ PCA
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

print(f"âœ… Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø§ PCA Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯ (ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {pca.explained_variance_ratio_.sum():.2%})")

# ========================
# 3. ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø±ÙˆØ´ Elbow
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 3: ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§")
print("=" * 60)

print("\nðŸ“Š Ø±ÙˆØ´ Elbow Method...")

# Ù…Ø­Ø§Ø³Ø¨Ù‡ WCSS Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø®ØªÙ„Ù Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
wcss = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Silhouette Score
    labels = kmeans.labels_
    silhouette_avg = silhouette_score(X_scaled, labels)
    silhouette_scores.append(silhouette_avg)

    print(f"  K={k}: WCSS={kmeans.inertia_:.2f}, Silhouette Score={silhouette_avg:.4f}")

# Ù†Ù…ÙˆØ¯Ø§Ø± Elbow
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Ù†Ù…ÙˆØ¯Ø§Ø± WCSS
ax1.plot(k_range, wcss, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ (K)')
ax1.set_ylabel('WCSS (Within-Cluster Sum of Squares)')
ax1.set_title('Elbow Method - WCSS')
ax1.grid(True, alpha=0.3)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø· Ø¹Ù…ÙˆØ¯ÛŒ Ø¯Ø± Ù†Ù‚Ø·Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ K=3 ÛŒØ§ 4)
optimal_k_elbow = 3  # Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ÛŒÙ† Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯
ax1.axvline(x=optimal_k_elbow, color='r', linestyle='--', alpha=0.5)
ax1.text(optimal_k_elbow, max(wcss) * 0.9, f'Optimal K={optimal_k_elbow}',
         rotation=90, verticalalignment='bottom')

# Ù†Ù…ÙˆØ¯Ø§Ø± Silhouette Score
ax2.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
ax2.set_xlabel('ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ (K)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Score Analysis')
ax2.grid(True, alpha=0.3)

# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† K Ø¨Ø± Ø§Ø³Ø§Ø³ Silhouette Score
optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
ax2.axvline(x=optimal_k_silhouette, color='r', linestyle='--', alpha=0.5)
ax2.text(optimal_k_silhouette, max(silhouette_scores) * 0.95,
         f'Best K={optimal_k_silhouette}', rotation=90, verticalalignment='bottom')

plt.tight_layout()
plt.savefig('elbow_silhouette_analysis.png', dpi=100, bbox_inches='tight')
print("\nâœ… Ù†Ù…ÙˆØ¯Ø§Ø± Elbow Ùˆ Silhouette Ø¯Ø± 'elbow_silhouette_analysis.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# Ø§Ù†ØªØ®Ø§Ø¨ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡
optimal_k = optimal_k_silhouette
print(f"\nðŸŽ¯ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§: {optimal_k}")

# ========================
# 4. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 4: Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

clustering_results = {}

# 4.1. K-Means
print("\nðŸ”„ 1. K-Means Clustering...")
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_scaled)

clustering_results['K-Means'] = {
    'labels': kmeans_labels,
    'silhouette': silhouette_score(X_scaled, kmeans_labels),
    'davies_bouldin': davies_bouldin_score(X_scaled, kmeans_labels),
    'calinski_harabasz': calinski_harabasz_score(X_scaled, kmeans_labels),
    'centers': kmeans.cluster_centers_
}

print(f"  âœ“ Silhouette Score: {clustering_results['K-Means']['silhouette']:.4f}")
print(f"  âœ“ Davies-Bouldin Score: {clustering_results['K-Means']['davies_bouldin']:.4f}")
print(f"  âœ“ Calinski-Harabasz Score: {clustering_results['K-Means']['calinski_harabasz']:.2f}")

# 4.2. DBSCAN
print("\nðŸ”„ 2. DBSCAN Clustering...")
# ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ DBSCAN
eps_values = np.arange(0.1, 2.0, 0.1)
min_samples_values = [3, 5, 10]

best_dbscan_score = -1
best_dbscan_params = {}

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan_labels = dbscan.fit_predict(X_scaled)

        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù‡ Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ 2 Ø®ÙˆØ´Ù‡ Ø¯Ø§Ø±ÛŒÙ…
        n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)

        if n_clusters >= 2:
            score = silhouette_score(X_scaled, dbscan_labels)
            if score > best_dbscan_score:
                best_dbscan_score = score
                best_dbscan_params = {'eps': eps, 'min_samples': min_samples}
                best_dbscan_labels = dbscan_labels

# Ø§Ø¬Ø±Ø§ÛŒ DBSCAN Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
if best_dbscan_params:
    dbscan = DBSCAN(**best_dbscan_params)
    dbscan_labels = dbscan.fit_predict(X_scaled)

    clustering_results['DBSCAN'] = {
        'labels': dbscan_labels,
        'silhouette': silhouette_score(X_scaled, dbscan_labels),
        'davies_bouldin': davies_bouldin_score(X_scaled, dbscan_labels),
        'calinski_harabasz': calinski_harabasz_score(X_scaled, dbscan_labels),
        'params': best_dbscan_params,
        'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0),
        'n_noise': list(dbscan_labels).count(-1)
    }

    print(f"  âœ“ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: eps={best_dbscan_params['eps']:.1f}, min_samples={best_dbscan_params['min_samples']}")
    print(f"  âœ“ ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§: {clustering_results['DBSCAN']['n_clusters']}")
    print(f"  âœ“ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ù†ÙˆÛŒØ²: {clustering_results['DBSCAN']['n_noise']}")
    print(f"  âœ“ Silhouette Score: {clustering_results['DBSCAN']['silhouette']:.4f}")

# 4.3. Hierarchical Clustering
print("\nðŸ”„ 3. Hierarchical (Agglomerative) Clustering...")
hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
hierarchical_labels = hierarchical.fit_predict(X_scaled)

clustering_results['Hierarchical'] = {
    'labels': hierarchical_labels,
    'silhouette': silhouette_score(X_scaled, hierarchical_labels),
    'davies_bouldin': davies_bouldin_score(X_scaled, hierarchical_labels),
    'calinski_harabasz': calinski_harabasz_score(X_scaled, hierarchical_labels)
}

print(f"  âœ“ Silhouette Score: {clustering_results['Hierarchical']['silhouette']:.4f}")
print(f"  âœ“ Davies-Bouldin Score: {clustering_results['Hierarchical']['davies_bouldin']:.4f}")

# 4.4. Gaussian Mixture Model
print("\nðŸ”„ 4. Gaussian Mixture Model...")
gmm = GaussianMixture(n_components=optimal_k, random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)

clustering_results['GMM'] = {
    'labels': gmm_labels,
    'silhouette': silhouette_score(X_scaled, gmm_labels),
    'davies_bouldin': davies_bouldin_score(X_scaled, gmm_labels),
    'calinski_harabasz': calinski_harabasz_score(X_scaled, gmm_labels),
    'probabilities': gmm.predict_proba(X_scaled)
}

print(f"  âœ“ Silhouette Score: {clustering_results['GMM']['silhouette']:.4f}")
print(f"  âœ“ Davies-Bouldin Score: {clustering_results['GMM']['davies_bouldin']:.4f}")

# ========================
# 5. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 5: Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

# Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
comparison_data = []
for algo_name, results in clustering_results.items():
    comparison_data.append({
        'Algorithm': algo_name,
        'Silhouette Score': results['silhouette'],
        'Davies-Bouldin Score': results['davies_bouldin'],
        'Calinski-Harabasz Score': results['calinski_harabasz']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('Silhouette Score', ascending=False)

print("\nðŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print("â”€" * 70)
print(comparison_df.to_string(index=False))

# Ø°Ø®ÛŒØ±Ù‡ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
comparison_df.to_csv('clustering_comparison.csv', index=False, encoding='utf-8-sig')
print("\nâœ… Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø± 'clustering_comparison.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…
best_algorithm = comparison_df.iloc[0]['Algorithm']
best_labels = clustering_results[best_algorithm]['labels']
print(f"\nðŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {best_algorithm}")

# ========================
# 6. ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 6: ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡")
print("=" * 60)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² K-Means Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
analysis_labels = clustering_results['K-Means']['labels']
df['Cluster'] = analysis_labels

print(f"\nðŸ“Š ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ K-Means:")
print("â”€" * 50)

# ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ø®ÙˆØ´Ù‡
cluster_analysis = []
for cluster_id in range(optimal_k):
    cluster_data = df[df['Cluster'] == cluster_id]

    print(f"\nðŸ” Ø®ÙˆØ´Ù‡ {cluster_id + 1}:")
    print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†: {len(cluster_data)} ({len(cluster_data) / len(df) * 100:.1f}%)")

    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
    print("  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:")

    important_features = ['AttendanceRate', 'StudyHoursPerWeek', 'PreviousGrade',
                          'ExtracurricularActivities', 'Study Hours']

    cluster_stats = {}
    for feature in feature_columns:
        if feature in cluster_data.columns:
            mean_val = cluster_data[feature].mean()
            cluster_stats[feature] = mean_val
            if feature in important_features:
                print(f"    - {feature}: {mean_val:.2f}")

    # Ø¯Ø±ØµØ¯ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø®ÙˆØ´Ù‡
    if 'Pass_Status_Encoded' in cluster_data.columns:
        pass_rate = cluster_data['Pass_Status_Encoded'].mean() * 100
        print(f"  â€¢ Ø¯Ø±ØµØ¯ Ù‚Ø¨ÙˆÙ„ÛŒ: {pass_rate:.1f}%")
        cluster_stats['Pass_Rate'] = pass_rate

    cluster_analysis.append({
        'Cluster': cluster_id + 1,
        'Size': len(cluster_data),
        'Percentage': len(cluster_data) / len(df) * 100,
        **cluster_stats
    })

# Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
cluster_analysis_df = pd.DataFrame(cluster_analysis)
cluster_analysis_df.to_csv('cluster_analysis.csv', index=False, encoding='utf-8-sig')
print("\nâœ… ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¯Ø± 'cluster_analysis.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# ========================
# 7. Visualization Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 7: Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

# Ù†Ù…ÙˆØ¯Ø§Ø± Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

algorithms = ['K-Means', 'DBSCAN', 'Hierarchical', 'GMM']
for idx, algo in enumerate(algorithms):
    if algo in clustering_results:
        labels = clustering_results[algo]['labels']

        # Ø±Ø³Ù… Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ¶Ø§ÛŒ PCA
        scatter = axes[idx].scatter(X_pca[:, 0], X_pca[:, 1],
                                    c=labels, cmap='viridis',
                                    s=50, alpha=0.6, edgecolors='black', linewidth=0.5)

        axes[idx].set_title(f'{algo} Clustering\n(Silhouette: {clustering_results[algo]["silhouette"]:.3f})')
        axes[idx].set_xlabel('First Principal Component')
        axes[idx].set_ylabel('Second Principal Component')

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ K-Means
        if algo == 'K-Means':
            centers_pca = pca.transform(clustering_results[algo]['centers'])
            axes[idx].scatter(centers_pca[:, 0], centers_pca[:, 1],
                              c='red', marker='X', s=200, edgecolors='black', linewidth=2)
            axes[idx].legend(['Clusters', 'Centers'], loc='upper right')

        plt.colorbar(scatter, ax=axes[idx])

plt.suptitle('Clustering Results Visualization (PCA Space)', fontsize=16)
plt.tight_layout()
plt.savefig('clustering_visualization.png', dpi=100, bbox_inches='tight')
print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± 'clustering_visualization.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# ========================
# 8. Dendrogram Ø¨Ø±Ø§ÛŒ Hierarchical Clustering
# ========================
print("\nðŸ“Š Ø±Ø³Ù… Dendrogram Ø¨Ø±Ø§ÛŒ Hierarchical Clustering...")

plt.figure(figsize=(15, 8))
Z = linkage(X_scaled, 'ward')
dendrogram(Z, truncate_mode='level', p=10)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index or (Cluster Size)')
plt.ylabel('Distance')
plt.axhline(y=Z[-optimal_k + 1, 2], color='r', linestyle='--', alpha=0.5)
plt.text(0, Z[-optimal_k + 1, 2], f'Cut for {optimal_k} clusters',
         verticalalignment='bottom', fontsize=10)
plt.tight_layout()
plt.savefig('dendrogram.png', dpi=100, bbox_inches='tight')
print("âœ… Dendrogram Ø¯Ø± 'dendrogram.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# ========================
# 9. ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 8: ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§")
print("=" * 60)

# Ù†Ù…ÙˆØ¯Ø§Ø± Box Plot Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
important_features_to_plot = ['AttendanceRate', 'StudyHoursPerWeek',
                              'PreviousGrade', 'ExtracurricularActivities']

# ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
available_features = [f for f in important_features_to_plot if f in df.columns]

if available_features:
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()

    for idx, feature in enumerate(available_features[:4]):
        cluster_data = []
        cluster_labels_plot = []

        for cluster_id in range(optimal_k):
            cluster_values = df[df['Cluster'] == cluster_id][feature].values
            cluster_data.append(cluster_values)
            cluster_labels_plot.append(f'Cluster {cluster_id + 1}')

        bp = axes[idx].boxplot(cluster_data, labels=cluster_labels_plot, patch_artist=True)

        # Ø±Ù†Ú¯â€ŒØ¢Ù…ÛŒØ²ÛŒ box plot
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, optimal_k))
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)

        axes[idx].set_title(f'{feature} Distribution by Cluster')
        axes[idx].set_ylabel(feature)
        axes[idx].grid(True, alpha=0.3)

    plt.suptitle('Feature Distribution Across Clusters', fontsize=16)
    plt.tight_layout()
    plt.savefig('cluster_feature_distribution.png', dpi=100, bbox_inches='tight')
    print("âœ… ØªÙˆØ²ÛŒØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± 'cluster_feature_distribution.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    plt.close()

# ========================
# 10. Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨ÙˆÙ„ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 9: Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨ÙˆÙ„ÛŒ")
print("=" * 60)

# Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
from sklearn.metrics import confusion_matrix as conf_matrix

# Ø§ÛŒØ¬Ø§Ø¯ confusion matrix
cm = conf_matrix(y_actual, analysis_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[f'Cluster {i + 1}' for i in range(optimal_k)],
            yticklabels=['Fail', 'Pass'])
plt.title('Confusion Matrix: Actual Pass/Fail vs Clusters')
plt.xlabel('Predicted Cluster')
plt.ylabel('Actual Pass/Fail Status')
plt.tight_layout()
plt.savefig('cluster_pass_fail_matrix.png', dpi=100, bbox_inches='tight')
print("âœ… Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¯Ø± 'cluster_pass_fail_matrix.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
plt.close()

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
ari = adjusted_rand_score(y_actual, analysis_labels)
ami = adjusted_mutual_info_score(y_actual, analysis_labels)

print(f"\nðŸ“Š Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨ÙˆÙ„ÛŒ:")
print(f"  â€¢ Adjusted Rand Index: {ari:.4f}")
print(f"  â€¢ Adjusted Mutual Information: {ami:.4f}")

# ========================
# 11. Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 10: Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡")
print("=" * 60)

print("\nðŸ“‹ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§:")
print("â”€" * 50)

for cluster_id in range(optimal_k):
    cluster_data = df[df['Cluster'] == cluster_id]

    print(f"\nâœ¨ Ø®ÙˆØ´Ù‡ {cluster_id + 1} - '{['Ø¶Ø¹ÛŒÙ', 'Ù…ØªÙˆØ³Ø·', 'Ù‚ÙˆÛŒ'][cluster_id % 3]}':")
    print(f"  ðŸ“Š ØªØ¹Ø¯Ø§Ø¯: {len(cluster_data)} Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²")

    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø¬Ø³ØªÙ‡
    if 'AttendanceRate' in cluster_data.columns:
        attendance_mean = cluster_data['AttendanceRate'].mean()
        print(f"  ðŸ“… Ø­Ø¶ÙˆØ±: {'Ø¨Ø§Ù„Ø§' if attendance_mean > 0.5 else 'Ù…ØªÙˆØ³Ø·' if attendance_mean > -0.5 else 'Ù¾Ø§ÛŒÛŒÙ†'}")

    if 'StudyHoursPerWeek' in cluster_data.columns:
        study_mean = cluster_data['StudyHoursPerWeek'].mean()
        print(f"  ðŸ“š Ø³Ø§Ø¹Ø§Øª Ù…Ø·Ø§Ù„Ø¹Ù‡: {'Ø²ÛŒØ§Ø¯' if study_mean > 0.5 else 'Ù…ØªÙˆØ³Ø·' if study_mean > -0.5 else 'Ú©Ù…'}")

    if 'Pass_Status_Encoded' in cluster_data.columns:
        pass_rate = cluster_data['Pass_Status_Encoded'].mean() * 100
        print(f"  ðŸŽ¯ Ù†Ø±Ø® Ù‚Ø¨ÙˆÙ„ÛŒ: {pass_rate:.1f}%")

    # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
    if pass_rate < 40:
        print("  ðŸ’¡ ØªÙˆØµÛŒÙ‡: Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ù…Ø§ÛŒØª ØªØ­ØµÛŒÙ„ÛŒ Ø¨ÛŒØ´ØªØ± Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ")
    elif pass_rate < 60:
        print("  ðŸ’¡ ØªÙˆØµÛŒÙ‡: ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ø§Ø¯Ø§Øª Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø²Ù…Ø§Ù†")
    else:
        print("  ðŸ’¡ ØªÙˆØµÛŒÙ‡: Ø­ÙØ¸ Ø±ÙˆÙ†Ø¯ ÙØ¹Ù„ÛŒ Ùˆ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±")

# ========================
# 12. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 60)
print("Ù…Ø±Ø­Ù„Ù‡ 11: Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 60)

# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
clustering_output = {
    'algorithms': clustering_results,
    'best_algorithm': best_algorithm,
    'optimal_k': optimal_k,
    'cluster_analysis': cluster_analysis_df,
    'evaluation_metrics': {
        'ARI': ari,
        'AMI': ami
    },
    'pca_components': X_pca,
    'feature_columns': feature_columns
}

with open('clustering_results.pkl', 'wb') as f:
    pickle.dump(clustering_output, f)

print("âœ… Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± 'clustering_results.pkl' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# Ø°Ø®ÛŒØ±Ù‡ DataFrame Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
df.to_csv('data_with_clusters.csv', index=False, encoding='utf-8-sig')
print("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ø®ÙˆØ´Ù‡ Ø¯Ø± 'data_with_clusters.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

# ========================
# 13. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
# ========================
print("\n" + "=" * 80)
print("âœ… ÙØ§Ø² 3: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
print("=" * 80)

print("\nðŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡:")
print("â”€" * 40)
print("1. âœ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
print("2. âœ“ ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Elbow Method Ùˆ Silhouette Score")
print(f"3. âœ“ Ø§Ø¬Ø±Ø§ÛŒ 4 Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø®ØªÙ„Ù")
print("4. âœ“ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Silhouette, Davies-Bouldin, Calinski-Harabasz")
print(f"5. âœ“ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {best_algorithm}")
print(f"6. âœ“ ØªØ­Ù„ÛŒÙ„ {optimal_k} Ø®ÙˆØ´Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡")
print("7. âœ“ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¨ÙˆÙ„ÛŒ/Ø±Ø¯")
print("8. âœ“ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡")
print("9. âœ“ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ ØªÙØ³ÛŒØ±ÛŒ")

print("\nðŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
print("â”€" * 40)
print("  â€¢ clustering_comparison.csv - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("  â€¢ cluster_analysis.csv - ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ù‡Ø± Ø®ÙˆØ´Ù‡")
print("  â€¢ data_with_clusters.csv - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ø®ÙˆØ´Ù‡")
print("  â€¢ clustering_results.pkl - Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("  â€¢ elbow_silhouette_analysis.png - Ù†Ù…ÙˆØ¯Ø§Ø± Elbow Ùˆ Silhouette")
print("  â€¢ clustering_visualization.png - Ù†Ù…Ø§ÛŒØ´ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ¶Ø§ÛŒ PCA")
print("  â€¢ dendrogram.png - Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ù„Ø³Ù„Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ÛŒ")
print("  â€¢ cluster_feature_distribution.png - ØªÙˆØ²ÛŒØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§")
print("  â€¢ cluster_pass_fail_matrix.png - Ù…Ø§ØªØ±ÛŒØ³ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù‚Ø¨ÙˆÙ„ÛŒ/Ø±Ø¯")

print("\nðŸ† Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒØ¯ÛŒ:")
print("â”€" * 40)
print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡: {optimal_k}")
print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {best_algorithm}")
print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Silhouette Score: {clustering_results[best_algorithm]['silhouette']:.4f}")
print(f"  â€¢ Adjusted Rand Index: {ari:.4f}")
print(f"  â€¢ Adjusted Mutual Information: {ami:.4f}")

print("\nðŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:")
print("â”€" * 40)
for i, cluster_info in enumerate(cluster_analysis):
    print(
        f"  â€¢ Ø®ÙˆØ´Ù‡ {i + 1}: {cluster_info['Size']} Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ² ({cluster_info['Percentage']:.1f}%) - Ù†Ø±Ø® Ù‚Ø¨ÙˆÙ„ÛŒ: {cluster_info.get('Pass_Rate', 0):.1f}%")

print("\nðŸŽ¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø² Ø¨Ø¹Ø¯ÛŒ:")
print("â”€" * 40)
print("  â†’ Association Rules (Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)")

print("\n" + "=" * 80)
print("Ù¾Ø§ÛŒØ§Ù† ÙØ§Ø² 3 - Clustering Students ðŸš€")
print("=" * 80)