# Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†
# ÙØ§Ø² 5: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Model Evaluation and Comparison)

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
from scipy import stats
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    silhouette_score, davies_bouldin_score, calinski_harabasz_score,
    mean_absolute_error, mean_squared_error, r2_score
)
from sklearn.model_selection import cross_val_score
import scipy.stats as st

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.precision', 4)

print("=" * 100)
print(" " * 30 + "ğŸ¯ ÙØ§Ø² 5: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
print(" " * 25 + "(Model Evaluation and Comparison)")
print("=" * 100)
print(f"\nğŸ“… ØªØ§Ø±ÛŒØ® Ø§Ø¬Ø±Ø§: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ========================
# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ ÙØ§Ø²Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
# ========================
print("\n" + "=" * 80)
print("ğŸ“‚ Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ ÙØ§Ø²Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ")
print("=" * 80)

# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬
all_results = {}

# 1.1 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Classification (ÙØ§Ø² 2)
try:
    with open('classification_results.pkl', 'rb') as f:
        classification_results = pickle.load(f)
    print("âœ… Ù†ØªØ§ÛŒØ¬ Classification Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    all_results['classification'] = classification_results

    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
    with open('all_models.pkl', 'rb') as f:
        trained_models = pickle.load(f)
    print("âœ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
except:
    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Classification")
    classification_results = {}
    trained_models = {}

# 1.2 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Clustering (ÙØ§Ø² 3)
try:
    with open('clustering_results.pkl', 'rb') as f:
        clustering_results = pickle.load(f)
    print("âœ… Ù†ØªØ§ÛŒØ¬ Clustering Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    all_results['clustering'] = clustering_results
except:
    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Clustering")
    clustering_results = {}

# 1.3 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Association Rules (ÙØ§Ø² 4)
try:
    with open('association_rules_summary.pkl', 'rb') as f:
        association_results = pickle.load(f)
    print("âœ… Ù†ØªØ§ÛŒØ¬ Association Rules Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    all_results['association'] = association_results

    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‚ÙˆØ§Ù†ÛŒÙ† Ú©Ø§Ù…Ù„
    rules_df = pd.read_csv('association_rules_all.csv')
    print(f"âœ… {len(rules_df)} Ù‚Ø§Ù†ÙˆÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
except:
    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Association Rules")
    association_results = {}
    rules_df = pd.DataFrame()

# 1.4 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
try:
    df_processed = pd.read_csv('processed_student_data.csv')
    df_with_clusters = pd.read_csv('data_with_clusters.csv')
    print(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ({len(df_processed)} Ø±Ú©ÙˆØ±Ø¯)")
except:
    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ")

print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡:")
print(f"  â€¢ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification: {len(classification_results)} Ù…Ø¯Ù„")
print(f"  â€¢ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Clustering: {len(clustering_results.get('algorithms', {}))} Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…")
print(f"  â€¢ Ù‚ÙˆØ§Ù†ÛŒÙ† Association: {association_results.get('total_rules', 0)} Ù‚Ø§Ù†ÙˆÙ†")

# ========================
# 2. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification")
print("=" * 80)

# Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
classification_comparison = []

for model_name, metrics in classification_results.items():
    comparison_entry = {
        'Model': model_name,
        'Accuracy': metrics.get('accuracy', 0),
        'Precision': metrics.get('precision', 0),
        'Recall': metrics.get('recall', 0),
        'F1-Score': metrics.get('f1_score', 0),
        'CV Mean': metrics.get('cv_mean', 0),
        'CV Std': metrics.get('cv_std', 0),
        'AUC': metrics.get('auc', 0)
    }
    classification_comparison.append(comparison_entry)

df_classification = pd.DataFrame(classification_comparison)
df_classification = df_classification.sort_values('F1-Score', ascending=False)

print("\nğŸ“ˆ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification:")
print("=" * 90)
print(df_classification.to_string(index=False))
print("=" * 90)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ
print("\nğŸ“Š Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
print("â”€" * 50)
stats_summary = df_classification[['Accuracy', 'Precision', 'Recall', 'F1-Score']].describe()
print(stats_summary.round(4))

# Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
best_model = df_classification.iloc[0]
print(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model['Model']}")
print(f"  â€¢ F1-Score: {best_model['F1-Score']:.4f}")
print(f"  â€¢ Accuracy: {best_model['Accuracy']:.4f}")
print(f"  â€¢ AUC: {best_model['AUC']:.4f}")

# ========================
# 3. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("=" * 80)

if 'algorithms' in clustering_results:
    clustering_comparison = []

    for algo_name, results in clustering_results['algorithms'].items():
        comparison_entry = {
            'Algorithm': algo_name,
            'Silhouette Score': results.get('silhouette', 0),
            'Davies-Bouldin': results.get('davies_bouldin', 0),
            'Calinski-Harabasz': results.get('calinski_harabasz', 0)
        }

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§Øµ DBSCAN
        if algo_name == 'DBSCAN':
            comparison_entry['Clusters'] = results.get('n_clusters', 0)
            comparison_entry['Noise Points'] = results.get('n_noise', 0)

        clustering_comparison.append(comparison_entry)

    df_clustering = pd.DataFrame(clustering_comparison)
    df_clustering = df_clustering.sort_values('Silhouette Score', ascending=False)

    print("\nğŸ“ˆ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Clustering:")
    print("=" * 80)
    print(df_clustering.to_string(index=False))
    print("=" * 80)

    # Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    best_clustering = df_clustering.iloc[0]
    print(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ: {best_clustering['Algorithm']}")
    print(f"  â€¢ Silhouette Score: {best_clustering['Silhouette Score']:.4f}")
    print(f"  â€¢ Davies-Bouldin: {best_clustering['Davies-Bouldin']:.4f}")

# ========================
# 4. ØªØ­Ù„ÛŒÙ„ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 4: ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ")
print("=" * 80)

if len(rules_df) > 0:
    print(f"\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ:")
    print("â”€" * 50)
    print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù‚ÙˆØ§Ù†ÛŒÙ†: {len(rules_df)}")
    print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Support: {rules_df['support'].mean():.4f}")
    print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Confidence: {rules_df['confidence'].mean():.4f}")
    print(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Lift: {rules_df['lift'].mean():.4f}")

    # Ù‚ÙˆØ§Ù†ÛŒÙ† Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ§Ø«ÛŒØ±
    top_impact_rules = rules_df.nlargest(10, 'lift')[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

    print("\nğŸ” 10 Ù‚Ø§Ù†ÙˆÙ† Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ§Ø«ÛŒØ± (Lift):")
    print("=" * 80)
    for idx, rule in top_impact_rules.iterrows():
        print(f"\nğŸ“Œ Ù‚Ø§Ù†ÙˆÙ† {idx + 1}:")
        print(f"  IF: {rule['antecedents']}")
        print(f"  THEN: {rule['consequents']}")
        print(f"  [Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}]")

# ========================
# 5. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†â€ŒÙØ§Ø²ÛŒ (Cross-Phase Comparison)
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 5: Ù…Ù‚Ø§ÛŒØ³Ù‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒÙ†â€ŒÙØ§Ø²ÛŒ")
print("=" * 80)

# Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù„ÛŒ
phase_comparison = {
    'Phase': ['Classification', 'Clustering', 'Association Rules'],
    'Best Method': [
        best_model['Model'] if 'best_model' in locals() else 'N/A',
        best_clustering['Algorithm'] if 'best_clustering' in locals() else 'N/A',
        'FP-Growth' if association_results else 'N/A'
    ],
    'Key Metric': [
        f"F1-Score: {best_model['F1-Score']:.4f}" if 'best_model' in locals() else 'N/A',
        f"Silhouette: {best_clustering['Silhouette Score']:.4f}" if 'best_clustering' in locals() else 'N/A',
        f"Rules: {association_results.get('total_rules', 0)}" if association_results else 'N/A'
    ],
    'Performance': [
        'Moderate' if 'best_model' in locals() and best_model['F1-Score'] < 0.6 else 'Good',
        'Good' if 'best_clustering' in locals() and best_clustering['Silhouette Score'] > 0.3 else 'Moderate',
        'Good' if association_results.get('strong_rules_count', 0) > 20 else 'Moderate'
    ]
}

df_phase_comparison = pd.DataFrame(phase_comparison)

print("\nğŸ“ˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙØ§Ø²Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù:")
print("=" * 70)
print(df_phase_comparison.to_string(index=False))
print("=" * 70)

# ========================
# 6. Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 6: Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ")
print("=" * 80)

# 6.1 Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification
if len(df_classification) > 0:
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Accuracy
    ax1 = axes[0, 0]
    models = df_classification['Model'].values
    accuracies = df_classification['Accuracy'].values
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(models)))
    bars1 = ax1.bar(range(len(models)), accuracies, color=colors)
    ax1.set_xlabel('Models', fontsize=12)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(len(models)))
    ax1.set_xticklabels(models, rotation=45, ha='right')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim([0, 1])

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§
    for i, (bar, val) in enumerate(zip(bars1, accuracies)):
        ax1.text(bar.get_x() + bar.get_width() / 2, val + 0.01,
                 f'{val:.3f}', ha='center', va='bottom', fontsize=9)

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ F1-Score
    ax2 = axes[0, 1]
    f1_scores = df_classification['F1-Score'].values
    bars2 = ax2.bar(range(len(models)), f1_scores, color=colors)
    ax2.set_xlabel('Models', fontsize=12)
    ax2.set_ylabel('F1-Score', fontsize=12)
    ax2.set_title('F1-Score Comparison', fontsize=14, fontweight='bold')
    ax2.set_xticks(range(len(models)))
    ax2.set_xticklabels(models, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0, 1])

    for i, (bar, val) in enumerate(zip(bars2, f1_scores)):
        ax2.text(bar.get_x() + bar.get_width() / 2, val + 0.01,
                 f'{val:.3f}', ha='center', va='bottom', fontsize=9)

    # Ù†Ù…ÙˆØ¯Ø§Ø± Radar Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¹ÛŒØ§Ø±Ù‡
    ax3 = axes[0, 2]

    # Ø§Ù†ØªØ®Ø§Ø¨ 5 Ù…Ø¯Ù„ Ø¨Ø±ØªØ±
    top_5_models = df_classification.head(5)
    categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]

    ax3 = plt.subplot(2, 3, 3, projection='polar')

    for idx, row in top_5_models.iterrows():
        values = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]
        values += values[:1]
        ax3.plot(angles, values, 'o-', linewidth=2, label=row['Model'][:15])
        ax3.fill(angles, values, alpha=0.1)

    ax3.set_xticks(angles[:-1])
    ax3.set_xticklabels(categories)
    ax3.set_ylim(0, 1)
    ax3.set_title('Top 5 Models - Multi-Metric Comparison',
                  fontsize=14, fontweight='bold', pad=20)
    ax3.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    ax3.grid(True)

    # Ù†Ù…ÙˆØ¯Ø§Ø± Box Plot Ø¨Ø±Ø§ÛŒ Cross-Validation Scores
    ax4 = axes[1, 0]
    cv_data = [df_classification['CV Mean'].values]
    bp = ax4.boxplot(cv_data, vert=True, patch_artist=True)
    bp['boxes'][0].set_facecolor('lightblue')
    ax4.set_ylabel('Cross-Validation Score', fontsize=12)
    ax4.set_title('Cross-Validation Score Distribution', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)

    # Scatter plot: Precision vs Recall
    ax5 = axes[1, 1]
    scatter = ax5.scatter(df_classification['Precision'],
                          df_classification['Recall'],
                          c=df_classification['F1-Score'],
                          s=100, cmap='viridis', alpha=0.7, edgecolors='black')

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡
    for idx, row in df_classification.iterrows():
        ax5.annotate(row['Model'][:10],
                     (row['Precision'], row['Recall']),
                     fontsize=8, ha='center')

    ax5.set_xlabel('Precision', fontsize=12)
    ax5.set_ylabel('Recall', fontsize=12)
    ax5.set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=ax5, label='F1-Score')

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ AUC
    ax6 = axes[1, 2]
    auc_values = df_classification['AUC'].values
    bars3 = ax6.barh(range(len(models)), auc_values, color=colors)
    ax6.set_xlabel('AUC Score', fontsize=12)
    ax6.set_ylabel('Models', fontsize=12)
    ax6.set_title('AUC-ROC Comparison', fontsize=14, fontweight='bold')
    ax6.set_yticks(range(len(models)))
    ax6.set_yticklabels(models)
    ax6.grid(True, alpha=0.3, axis='x')
    ax6.set_xlim([0, 1])

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ±
    for i, (bar, val) in enumerate(zip(bars3, auc_values)):
        ax6.text(val + 0.01, bar.get_y() + bar.get_height() / 2,
                 f'{val:.3f}', ha='left', va='center', fontsize=9)

    plt.suptitle('Classification Models - Comprehensive Evaluation',
                 fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('classification_comprehensive_evaluation.png', dpi=150, bbox_inches='tight')
    print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø§Ù…Ø¹ Classification Ø¯Ø± 'classification_comprehensive_evaluation.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    plt.close()

# 6.2 Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
if len(df_clustering) > 0:
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Silhouette Score
    ax1 = axes[0]
    algorithms = df_clustering['Algorithm'].values
    silhouette_scores = df_clustering['Silhouette Score'].values
    colors_cluster = plt.cm.coolwarm(np.linspace(0.2, 0.8, len(algorithms)))
    bars1 = ax1.bar(algorithms, silhouette_scores, color=colors_cluster)
    ax1.set_xlabel('Algorithm', fontsize=12)
    ax1.set_ylabel('Silhouette Score', fontsize=12)
    ax1.set_title('Silhouette Score Comparison', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)

    for bar, val in zip(bars1, silhouette_scores):
        ax1.text(bar.get_x() + bar.get_width() / 2, val + 0.01,
                 f'{val:.3f}', ha='center', va='bottom', fontsize=10)

    # Davies-Bouldin Score (lower is better)
    ax2 = axes[1]
    db_scores = df_clustering['Davies-Bouldin'].values
    bars2 = ax2.bar(algorithms, db_scores, color=colors_cluster)
    ax2.set_xlabel('Algorithm', fontsize=12)
    ax2.set_ylabel('Davies-Bouldin Score', fontsize=12)
    ax2.set_title('Davies-Bouldin Score (Lower is Better)', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    for bar, val in zip(bars2, db_scores):
        ax2.text(bar.get_x() + bar.get_width() / 2, val + 0.05,
                 f'{val:.3f}', ha='center', va='bottom', fontsize=10)

    # Calinski-Harabasz Score
    ax3 = axes[2]
    ch_scores = df_clustering['Calinski-Harabasz'].values
    bars3 = ax3.bar(algorithms, ch_scores, color=colors_cluster)
    ax3.set_xlabel('Algorithm', fontsize=12)
    ax3.set_ylabel('Calinski-Harabasz Score', fontsize=12)
    ax3.set_title('Calinski-Harabasz Score (Higher is Better)', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)

    for bar, val in zip(bars3, ch_scores):
        ax3.text(bar.get_x() + bar.get_width() / 2, val + 1,
                 f'{val:.1f}', ha='center', va='bottom', fontsize=10)

    plt.suptitle('Clustering Algorithms - Quality Metrics Comparison',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('clustering_evaluation.png', dpi=150, bbox_inches='tight')
    print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Clustering Ø¯Ø± 'clustering_evaluation.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    plt.close()

# 6.3 Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ­Ù„ÛŒÙ„ Association Rules
if len(rules_df) > 0:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # ØªÙˆØ²ÛŒØ¹ Support
    ax1 = axes[0, 0]
    ax1.hist(rules_df['support'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
    ax1.set_xlabel('Support', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.set_title('Distribution of Support Values', fontsize=14, fontweight='bold')
    ax1.axvline(rules_df['support'].mean(), color='red', linestyle='--',
                label=f'Mean: {rules_df["support"].mean():.3f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # ØªÙˆØ²ÛŒØ¹ Confidence
    ax2 = axes[0, 1]
    ax2.hist(rules_df['confidence'], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)
    ax2.set_xlabel('Confidence', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('Distribution of Confidence Values', fontsize=14, fontweight='bold')
    ax2.axvline(rules_df['confidence'].mean(), color='red', linestyle='--',
                label=f'Mean: {rules_df["confidence"].mean():.3f}')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # ØªÙˆØ²ÛŒØ¹ Lift
    ax3 = axes[1, 0]
    ax3.hist(rules_df['lift'], bins=30, color='coral', edgecolor='black', alpha=0.7)
    ax3.set_xlabel('Lift', fontsize=12)
    ax3.set_ylabel('Frequency', fontsize=12)
    ax3.set_title('Distribution of Lift Values', fontsize=14, fontweight='bold')
    ax3.axvline(1, color='green', linestyle='--', label='Lift = 1 (Independence)')
    ax3.axvline(rules_df['lift'].mean(), color='red', linestyle='--',
                label=f'Mean: {rules_df["lift"].mean():.3f}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # Scatter: Support vs Confidence colored by Lift
    ax4 = axes[1, 1]
    scatter = ax4.scatter(rules_df['support'], rules_df['confidence'],
                          c=rules_df['lift'], cmap='viridis',
                          s=50, alpha=0.6, edgecolors='black', linewidth=0.5)
    ax4.set_xlabel('Support', fontsize=12)
    ax4.set_ylabel('Confidence', fontsize=12)
    ax4.set_title('Support vs Confidence (colored by Lift)', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=ax4, label='Lift')

    plt.suptitle('Association Rules - Statistical Analysis',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('association_rules_analysis.png', dpi=150, bbox_inches='tight')
    print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ­Ù„ÛŒÙ„ Association Rules Ø¯Ø± 'association_rules_analysis.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    plt.close()

# ========================
# 7. ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 7: ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡")
print("=" * 80)

# 7.1 Ø¢Ø²Ù…ÙˆÙ† Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
if len(df_classification) > 1:
    print("\nğŸ“ˆ Ø¢Ø²Ù…ÙˆÙ† Friedman Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification:")
    print("â”€" * 50)

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†
    metrics_for_test = df_classification[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values

    # Ø§Ù†Ø¬Ø§Ù… Ø¢Ø²Ù…ÙˆÙ† Friedman
    if metrics_for_test.shape[0] >= 3:
        statistic, p_value = stats.friedmanchisquare(*metrics_for_test)
        print(f"  â€¢ Friedman Statistic: {statistic:.4f}")
        print(f"  â€¢ P-value: {p_value:.6f}")

        if p_value < 0.05:
            print("  âœ… Ù†ØªÛŒØ¬Ù‡: ØªÙØ§ÙˆØª Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ (p < 0.05)")
        else:
            print("  âš ï¸ Ù†ØªÛŒØ¬Ù‡: ØªÙØ§ÙˆØª Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ (p >= 0.05)")

# 7.2 Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
print("\nğŸ“ˆ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† 95% Ø¨Ø±Ø§ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
print("â”€" * 50)

for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
    values = df_classification[metric].values
    mean = np.mean(values)
    std = np.std(values)
    n = len(values)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
    confidence_level = 0.95
    degrees_freedom = n - 1
    t_value = st.t.ppf((1 + confidence_level) / 2, degrees_freedom)
    margin_error = t_value * (std / np.sqrt(n))

    ci_lower = mean - margin_error
    ci_upper = mean + margin_error

    print(f"  â€¢ {metric}: [{ci_lower:.4f}, {ci_upper:.4f}] (Mean: {mean:.4f})")

# ========================
# 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 8: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
print("=" * 80)

# 8.1 Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø±ÙˆØ´
print("\nğŸ“ˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ (Composite Score):")
print("â”€" * 50)

# Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Classification
if len(df_classification) > 0:
    classification_score = (
            df_classification['F1-Score'].mean() * 0.4 +
            df_classification['Accuracy'].mean() * 0.3 +
            df_classification['AUC'].mean() * 0.3
    )
    print(f"  â€¢ Classification Composite Score: {classification_score:.4f}")
else:
    classification_score = 0

# Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Clustering
if len(df_clustering) > 0:
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Silhouette (0 to 1)
    silhouette_norm = (df_clustering['Silhouette Score'].mean() + 1) / 2
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Davies-Bouldin (inverse, lower is better)
    db_norm = 1 / (1 + df_clustering['Davies-Bouldin'].mean())
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Calinski-Harabasz
    ch_max = df_clustering['Calinski-Harabasz'].max()
    ch_norm = df_clustering['Calinski-Harabasz'].mean() / ch_max if ch_max > 0 else 0

    clustering_score = (silhouette_norm * 0.5 + db_norm * 0.3 + ch_norm * 0.2)
    print(f"  â€¢ Clustering Composite Score: {clustering_score:.4f}")
else:
    clustering_score = 0

# Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Association Rules
if association_results:
    total_rules = association_results.get('total_rules', 0)
    strong_rules = association_results.get('strong_rules_count', 0)

    if total_rules > 0:
        rule_quality = strong_rules / total_rules
        rule_quantity = min(total_rules / 100, 1)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ù†ÛŒÙ†
        association_score = (rule_quality * 0.6 + rule_quantity * 0.4)
    else:
        association_score = 0

    print(f"  â€¢ Association Rules Composite Score: {association_score:.4f}")
else:
    association_score = 0

# 8.2 Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
print("\nğŸ† Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§:")
print("â”€" * 50)

methods_ranking = {
    'Classification': classification_score,
    'Clustering': clustering_score,
    'Association Rules': association_score
}

sorted_methods = sorted(methods_ranking.items(), key=lambda x: x[1], reverse=True)

for rank, (method, score) in enumerate(sorted_methods, 1):
    medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
    print(f"  {medal} Ø±ØªØ¨Ù‡ {rank}: {method} (Score: {score:.4f})")

# ========================
# 9. ØªØ­Ù„ÛŒÙ„ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù
# ========================
print("\n" + "=" * 80)
print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 9: ØªØ­Ù„ÛŒÙ„ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù Ù‡Ø± Ø±ÙˆØ´")
print("=" * 80)

# 9.1 ØªØ­Ù„ÛŒÙ„ Classification
print("\nğŸ“Œ Classification Models:")
print("â”€" * 50)
if len(df_classification) > 0:
    avg_f1 = df_classification['F1-Score'].mean()

    print("âœ… Ù†Ù‚Ø§Ø· Ù‚ÙˆØª:")
    if avg_f1 > 0.7:
        print("  â€¢ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (F1 > 0.7)")
    elif avg_f1 > 0.5:
        print("  â€¢ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ")

    if df_classification['Precision'].mean() > df_classification['Recall'].mean():
        print("  â€¢ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù…ÙˆØ§Ø±Ø¯ Ù…Ø«Ø¨Øª")
    else:
        print("  â€¢ Ù¾ÙˆØ´Ø´ Ø®ÙˆØ¨ Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÙ…Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ Ù…Ø«Ø¨Øª")

    print("\nâš ï¸ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù:")
    if avg_f1 < 0.6:
        print("  â€¢ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‚Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ")

    cv_std_avg = df_classification['CV Std'].mean()
    if cv_std_avg > 0.05:
        print(f"  â€¢ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø§Ù„Ø§ Ø¯Ø± Cross-Validation (Std: {cv_std_avg:.4f})")

# 9.2 ØªØ­Ù„ÛŒÙ„ Clustering
print("\nğŸ“Œ Clustering Algorithms:")
print("â”€" * 50)
if len(df_clustering) > 0:
    avg_silhouette = df_clustering['Silhouette Score'].mean()

    print("âœ… Ù†Ù‚Ø§Ø· Ù‚ÙˆØª:")
    if avg_silhouette > 0.5:
        print("  â€¢ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¹Ø§Ù„ÛŒ (Silhouette > 0.5)")
    elif avg_silhouette > 0.25:
        print("  â€¢ Ø³Ø§Ø®ØªØ§Ø± Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„")

    if clustering_results.get('optimal_k', 0) <= 3:
        print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ØªÙØ³ÛŒØ± ({clustering_results.get('optimal_k', 'N/A')})")

    print("\nâš ï¸ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù:")
    if avg_silhouette < 0.25:
        print("  â€¢ Ø³Ø§Ø®ØªØ§Ø± Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¶Ø¹ÛŒÙ")

    if 'evaluation_metrics' in clustering_results:
        ari = clustering_results['evaluation_metrics'].get('ARI', 0)
        if ari < 0.3:
            print(f"  â€¢ Ø§Ø±ØªØ¨Ø§Ø· Ø¶Ø¹ÛŒÙ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ (ARI: {ari:.4f})")

# 9.3 ØªØ­Ù„ÛŒÙ„ Association Rules
print("\nğŸ“Œ Association Rules:")
print("â”€" * 50)
if association_results:
    print("âœ… Ù†Ù‚Ø§Ø· Ù‚ÙˆØª:")
    if association_results.get('total_rules', 0) > 50:
        print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ù‚ÙˆØ§Ù†ÛŒÙ† Ú©Ø´Ù Ø´Ø¯Ù‡ ({association_results.get('total_rules', 0)})")

    if association_results.get('strong_rules_count', 0) > 10:
        print(f"  â€¢ ÙˆØ¬ÙˆØ¯ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‚ÙˆÛŒ Ùˆ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± ({association_results.get('strong_rules_count', 0)})")

    print("\nâš ï¸ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù:")
    if association_results.get('total_rules', 0) < 20:
        print("  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù… Ù‚ÙˆØ§Ù†ÛŒÙ† Ú©Ø´Ù Ø´Ø¯Ù‡")

    if association_results.get('min_support', 1) < 0.05:
        print(f"  â€¢ Support Ù¾Ø§ÛŒÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ù‚ÙˆØ§Ù†ÛŒÙ† (Min: {association_results.get('min_support', 0):.3f})")

# ========================
# 10. ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯
# ========================
print("\n" + "=" * 80)
print("ğŸ’¡ Ù…Ø±Ø­Ù„Ù‡ 10: ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯")
print("=" * 80)

recommendations = []

# ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Classification
if len(df_classification) > 0:
    if df_classification['F1-Score'].mean() < 0.7:
        recommendations.append({
            'Ø±ÙˆØ´': 'Classification',
            'ØªÙˆØµÛŒÙ‡': 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ensemble Learning Ù…Ø§Ù†Ù†Ø¯ Stacking ÛŒØ§ Voting',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ø¨Ø§Ù„Ø§'
        })
        recommendations.append({
            'Ø±ÙˆØ´': 'Classification',
            'ØªÙˆØµÛŒÙ‡': 'Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Hyperparameters Ø¨Ø§ Bayesian Optimization',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ù…ØªÙˆØ³Ø·'
        })

    if abs(df_classification['Precision'].mean() - df_classification['Recall'].mean()) > 0.1:
        recommendations.append({
            'Ø±ÙˆØ´': 'Classification',
            'ØªÙˆØµÛŒÙ‡': 'ØªÙ†Ø¸ÛŒÙ… Threshold Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆØ§Ø²Ù† Ú©Ø±Ø¯Ù† Precision Ùˆ Recall',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ø¨Ø§Ù„Ø§'
        })

# ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Clustering
if len(df_clustering) > 0:
    if df_clustering['Silhouette Score'].mean() < 0.3:
        recommendations.append({
            'Ø±ÙˆØ´': 'Clustering',
            'ØªÙˆØµÛŒÙ‡': 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ù‚Ø¨Ù„ Ø§Ø² Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (PCA, t-SNE)',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ø¨Ø§Ù„Ø§'
        })
        recommendations.append({
            'Ø±ÙˆØ´': 'Clustering',
            'ØªÙˆØµÛŒÙ‡': 'Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ÙØ§ØµÙ„Ù‡ Ù…Ø®ØªÙ„Ù (Manhattan, Cosine)',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ù…ØªÙˆØ³Ø·'
        })

# ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Association Rules
if association_results:
    if association_results.get('total_rules', 0) < 50:
        recommendations.append({
            'Ø±ÙˆØ´': 'Association Rules',
            'ØªÙˆØµÛŒÙ‡': 'Ú©Ø§Ù‡Ø´ Ø­Ø¯ Ø¢Ø³ØªØ§Ù†Ù‡ Support Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ù‚ÙˆØ§Ù†ÛŒÙ† Ø¨ÛŒØ´ØªØ±',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ù…ØªÙˆØ³Ø·'
        })

    if association_results.get('strong_rules_count', 0) < 10:
        recommendations.append({
            'Ø±ÙˆØ´': 'Association Rules',
            'ØªÙˆØµÛŒÙ‡': 'Ø¨Ø±Ø±Ø³ÛŒ ØªØ±Ú©ÛŒØ¨Ø§Øª Ø¨ÛŒØ´ØªØ± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Max Length',
            'Ø§ÙˆÙ„ÙˆÛŒØª': 'Ù¾Ø§ÛŒÛŒÙ†'
        })

# Ù†Ù…Ø§ÛŒØ´ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
if recommendations:
    df_recommendations = pd.DataFrame(recommendations)

    print("\nğŸ“‹ Ø¬Ø¯ÙˆÙ„ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯:")
    print("=" * 80)

    for priority in ['Ø¨Ø§Ù„Ø§', 'Ù…ØªÙˆØ³Ø·', 'Ù¾Ø§ÛŒÛŒÙ†']:
        priority_recs = df_recommendations[df_recommendations['Ø§ÙˆÙ„ÙˆÛŒØª'] == priority]
        if not priority_recs.empty:
            print(f"\nğŸ”´ Ø§ÙˆÙ„ÙˆÛŒØª {priority}:")
            for _, rec in priority_recs.iterrows():
                print(f"  â€¢ [{rec['Ø±ÙˆØ´']}] {rec['ØªÙˆØµÛŒÙ‡']}")

# ========================
# 11. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¬Ø§Ù…Ø¹
# ========================
print("\n" + "=" * 80)
print("ğŸ“„ Ù…Ø±Ø­Ù„Ù‡ 11: ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¬Ø§Ù…Ø¹")
print("=" * 80)

# Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
final_report = []
final_report.append("=" * 100)
final_report.append(" " * 35 + "Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
final_report.append(" " * 30 + "Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†")
final_report.append("=" * 100)
final_report.append(f"\nØªØ§Ø±ÛŒØ® ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Ø¨Ø®Ø´ 1: Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ
final_report.append("\n" + "â”€" * 80)
final_report.append("1. Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ")
final_report.append("â”€" * 80)
final_report.append(
    f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡: {len(df_classification) if 'df_classification' in locals() else 0}")
final_report.append(
    f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Clustering Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡: {len(df_clustering) if 'df_clustering' in locals() else 0}")
final_report.append(
    f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ù†ÛŒÙ† Association Ú©Ø´Ù Ø´Ø¯Ù‡: {association_results.get('total_rules', 0) if association_results else 0}")

# Ø¨Ø®Ø´ 2: Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬
final_report.append("\n" + "â”€" * 80)
final_report.append("2. Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ù‡Ø± Ø±ÙˆØ´")
final_report.append("â”€" * 80)

if 'best_model' in locals():
    final_report.append(f"\nğŸ† Classification:")
    final_report.append(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model['Model']}")
    final_report.append(f"  â€¢ F1-Score: {best_model['F1-Score']:.4f}")
    final_report.append(f"  â€¢ Accuracy: {best_model['Accuracy']:.4f}")

if 'best_clustering' in locals():
    final_report.append(f"\nğŸ† Clustering:")
    final_report.append(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {best_clustering['Algorithm']}")
    final_report.append(f"  â€¢ Silhouette Score: {best_clustering['Silhouette Score']:.4f}")

if association_results:
    final_report.append(f"\nğŸ† Association Rules:")
    final_report.append(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‚ÙˆÛŒ: {association_results.get('strong_rules_count', 0)}")
    final_report.append(
        f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Lift: {rules_df['lift'].mean():.4f}" if len(rules_df) > 0 else "  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Lift: N/A")

# Ø¨Ø®Ø´ 3: Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
final_report.append("\n" + "â”€" * 80)
final_report.append("3. Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§")
final_report.append("â”€" * 80)

for rank, (method, score) in enumerate(sorted_methods, 1):
    final_report.append(f"  {rank}. {method}: {score:.4f}")

# Ø¨Ø®Ø´ 4: ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
final_report.append("\n" + "â”€" * 80)
final_report.append("4. ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ")
final_report.append("â”€" * 80)

if recommendations:
    high_priority = [r for r in recommendations if r['Ø§ÙˆÙ„ÙˆÛŒØª'] == 'Ø¨Ø§Ù„Ø§']
    if high_priority:
        final_report.append("\nâ€¢ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§:")
        for rec in high_priority[:5]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 ØªÙˆØµÛŒÙ‡
            final_report.append(f"  - [{rec['Ø±ÙˆØ´']}] {rec['ØªÙˆØµÛŒÙ‡']}")

# Ø¨Ø®Ø´ 5: Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
final_report.append("\n" + "â”€" * 80)
final_report.append("5. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ")
final_report.append("â”€" * 80)

# ØªØ¹ÛŒÛŒÙ† ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
overall_performance = (classification_score + clustering_score + association_score) / 3

if overall_performance > 0.7:
    performance_level = "Ø¹Ø§Ù„ÛŒ"
    emoji = "ğŸŒŸ"
elif overall_performance > 0.5:
    performance_level = "Ø®ÙˆØ¨"
    emoji = "âœ…"
elif overall_performance > 0.3:
    performance_level = "Ù…ØªÙˆØ³Ø·"
    emoji = "âš ï¸"
else:
    performance_level = "Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯"
    emoji = "ğŸ”´"

final_report.append(f"\n{emoji} Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…: {performance_level} (Ø§Ù…ØªÛŒØ§Ø²: {overall_performance:.4f})")
final_report.append("\nÙ¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± 5 ÙØ§Ø² Ø§Ø¬Ø±Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯.")
final_report.append("Ù†ØªØ§ÛŒØ¬ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ ØªØ±Ú©ÛŒØ¨ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø¯Ù‡â€ŒÚ©Ø§ÙˆÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯")
final_report.append("Ø¯ÛŒØ¯Ú¯Ø§Ù‡ Ø¬Ø§Ù…Ø¹ÛŒ Ø§Ø² Ø¹ÙˆØ§Ù…Ù„ Ù…ÙˆØ«Ø± Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªØ­ØµÛŒÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯.")

# Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
with open('final_evaluation_report.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(final_report))

print("âœ… Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± 'final_evaluation_report.txt' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
evaluation_results = {
    'classification_comparison': df_classification.to_dict() if 'df_classification' in locals() else {},
    'clustering_comparison': df_clustering.to_dict() if 'df_clustering' in locals() else {},
    'association_summary': association_results if association_results else {},
    'composite_scores': {
        'classification': classification_score,
        'clustering': clustering_score,
        'association': association_score
    },
    'overall_performance': overall_performance,
    'recommendations': recommendations,
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

with open('phase5_evaluation_results.pkl', 'wb') as f:
    pickle.dump(evaluation_results, f)

print("âœ… Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø± 'phase5_evaluation_results.pkl' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

# ========================
# 12. Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
# ========================
print("\nğŸ“Š Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Ù†Ù…ÙˆØ¯Ø§Ø± 1: Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª ØªØ±Ú©ÛŒØ¨ÛŒ
ax1 = axes[0, 0]
methods = list(methods_ranking.keys())
scores = list(methods_ranking.values())
colors_final = ['#FF6B6B', '#4ECDC4', '#45B7D1']
bars = ax1.bar(methods, scores, color=colors_final, edgecolor='black', linewidth=2)

for bar, score in zip(bars, scores):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
             f'{score:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

ax1.set_ylabel('Composite Score', fontsize=12, fontweight='bold')
ax1.set_title('Overall Performance Comparison', fontsize=14, fontweight='bold')
ax1.set_ylim([0, max(scores) * 1.2 if scores else 1])
ax1.grid(True, alpha=0.3, axis='y')

# Ù†Ù…ÙˆØ¯Ø§Ø± 2: Ø®Ù„Ø§ØµÙ‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Classification
ax2 = axes[0, 1]
if len(df_classification) > 0:
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    avg_values = [df_classification[m].mean() for m in metrics]

    radar_angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    radar_angles += radar_angles[:1]
    avg_values += avg_values[:1]

    ax2 = plt.subplot(2, 2, 2, projection='polar')
    ax2.plot(radar_angles, avg_values, 'o-', linewidth=2, color='#FF6B6B')
    ax2.fill(radar_angles, avg_values, alpha=0.25, color='#FF6B6B')
    ax2.set_xticks(radar_angles[:-1])
    ax2.set_xticklabels(metrics)
    ax2.set_ylim(0, 1)
    ax2.set_title('Classification Metrics Summary', fontsize=14, fontweight='bold', pad=20)
    ax2.grid(True)

# Ù†Ù…ÙˆØ¯Ø§Ø± 3: ØªÙˆØ²ÛŒØ¹ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø¨Ø± Ø§Ø³Ø§Ø³ Lift
ax3 = axes[1, 0]
if len(rules_df) > 0:
    lift_categories = ['Low (< 1)', 'Medium (1-1.5)', 'High (> 1.5)']
    lift_counts = [
        len(rules_df[rules_df['lift'] < 1]),
        len(rules_df[(rules_df['lift'] >= 1) & (rules_df['lift'] <= 1.5)]),
        len(rules_df[rules_df['lift'] > 1.5])
    ]

    colors_lift = ['#FF6B6B', '#FFD93D', '#6BCF7F']
    wedges, texts, autotexts = ax3.pie(lift_counts, labels=lift_categories, colors=colors_lift,
                                       autopct='%1.1f%%', startangle=90)
    ax3.set_title('Association Rules Distribution by Lift', fontsize=14, fontweight='bold')

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')

# Ù†Ù…ÙˆØ¯Ø§Ø± 4: Performance Level Gauge
ax4 = axes[1, 1]
ax4.axis('off')

# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© gauge chart Ø³Ø§Ø¯Ù‡
theta = np.linspace(0.2 * np.pi, 1.8 * np.pi, 100)
r_inner = 0.7
r_outer = 1.0

# Ø±Ø³Ù… Ù‚ÙˆØ³â€ŒÙ‡Ø§ÛŒ Ø±Ù†Ú¯ÛŒ
colors_gauge = ['#FF6B6B', '#FFD93D', '#6BCF7F']
boundaries = [0.2 * np.pi, 0.73 * np.pi, 1.27 * np.pi, 1.8 * np.pi]

for i in range(3):
    theta_section = np.linspace(boundaries[i], boundaries[i + 1], 30)
    ax4.fill_between(theta_section, r_inner, r_outer, color=colors_gauge[i], alpha=0.6)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø´Ø§Ù†Ú¯Ø±
indicator_angle = 0.2 * np.pi + overall_performance * 1.6 * np.pi
ax4.arrow(0, 0, 0.9 * np.cos(indicator_angle), 0.9 * np.sin(indicator_angle),
          head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)

ax4.set_xlim(-1.2, 1.2)
ax4.set_ylim(-0.2, 1.2)
ax4.text(0, -0.1, f'Overall Score: {overall_performance:.2%}',
         ha='center', fontsize=14, fontweight='bold')
ax4.text(0, 0.5, performance_level, ha='center', fontsize=18, fontweight='bold')
ax4.set_title('Overall System Performance', fontsize=14, fontweight='bold')

plt.suptitle('ğŸ“Š Comprehensive Evaluation Summary - Student Performance Prediction Project',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('final_evaluation_summary.png', dpi=150, bbox_inches='tight')
print("âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± 'final_evaluation_summary.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
plt.close()

# ========================
# Ù¾Ø§ÛŒØ§Ù† ÙØ§Ø² 5
# ========================
print("\n" + "=" * 100)
print("âœ… ÙØ§Ø² 5: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
print("=" * 100)

print("\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡:")
print("â”€" * 50)
print("1. âœ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ 4 ÙØ§Ø² Ù‚Ø¨Ù„ÛŒ")
print("2. âœ“ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Classification")
print("3. âœ“ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("4. âœ“ ØªØ­Ù„ÛŒÙ„ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ")
print("5. âœ“ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†â€ŒÙØ§Ø²ÛŒ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª ØªØ±Ú©ÛŒØ¨ÛŒ")
print("6. âœ“ ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡")
print("7. âœ“ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù")
print("8. âœ“ Ø§Ø±Ø§Ø¦Ù‡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯")
print("9. âœ“ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹")
print("10. âœ“ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ")

print("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
print("â”€" * 50)
print("  â€¢ final_evaluation_report.txt - Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¬Ø§Ù…Ø¹")
print("  â€¢ phase5_evaluation_results.pkl - Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
print("  â€¢ classification_comprehensive_evaluation.png - Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø§Ù…Ø¹ Classification")
print("  â€¢ clustering_evaluation.png - Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Clustering")
print("  â€¢ association_rules_analysis.png - Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ­Ù„ÛŒÙ„ Association Rules")
print("  â€¢ final_evaluation_summary.png - Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ")

print("\nğŸ† Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒØ¯ÛŒ:")
print("â”€" * 50)
print(f"  â€¢ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…: {performance_level} ({overall_performance:.4f})")
print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´: {sorted_methods[0][0]} (Ø§Ù…ØªÛŒØ§Ø²: {sorted_methods[0][1]:.4f})")
if 'best_model' in locals():
    print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Classification: {best_model['Model']}")
if 'best_clustering' in locals():
    print(f"  â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Clustering: {best_clustering['Algorithm']}")

print("\n" + "=" * 100)
print("ğŸ“ Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†")
print("ğŸ“Š ØªÙ…Ø§Ù… 5 ÙØ§Ø² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
print("=" * 100)

print("\nğŸš€ ÙØ§Ø²Ù‡Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡:")
print("  âœ… ÙØ§Ø² 1: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
print("  âœ… ÙØ§Ø² 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ")
print("  âœ… ÙØ§Ø² 3: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†")
print("  âœ… ÙØ§Ø² 4: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ")
print("  âœ… ÙØ§Ø² 5: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§")

print("\nğŸ’¡ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ Ú©Ù‡ ØªØ±Ú©ÛŒØ¨ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø¯Ù‡â€ŒÚ©Ø§ÙˆÛŒ")
print("   Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªØ­ØµÛŒÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯.")

print("\n" + "=" * 100)